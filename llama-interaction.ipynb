{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Llama 3.1 8B Model and Coding Dataset\n",
    "\n",
    "This notebook loads the Llama 3.1 8B model and the coding dataset, then allows you to run prompts through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "from data_loader import load_or_download_llama_model, load_or_download_coding_dataset\n",
    "import torch\n",
    "from transformers import TextGenerationPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama 3.2 1B Instruct model from local storage...\n",
      "Llama 3.2 1B Instruct model loaded successfully.\n",
      "Loading coding dataset from local storage...\n",
      "Coding dataset loaded successfully.\n",
      "Dataset size: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load the model and dataset\n",
    "tokenizer, model = load_or_download_llama_model()\n",
    "\n",
    "dataset = load_or_download_coding_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on a prompt\n",
    "def generate_text(prompt, max_length=100):\n",
    "    generated = generator(prompt, max_length=max_length, do_sample=True, top_k=50, top_p=0.95)\n",
    "    return generated[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "###############################################################################\n",
      "##\n",
      "##  Copyright (C) 2013-2014 Tavendo GmbH\n",
      "##\n",
      "##  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "##  y...\n",
      "\n",
      "Example 2:\n",
      "from itertools import chain\n",
      "\n",
      "from django.utils.itercompat import is_iterable\n",
      "\n",
      "\n",
      "class Tags:\n",
      "    \"\"\"\n",
      "    Built-in tags for internal checks.\n",
      "    \"\"\"\n",
      "    admin = 'admin'\n",
      "    caches = 'caches'\n",
      "    compatib...\n",
      "\n",
      "Example 3:\n",
      "\"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm...\n",
      "\n",
      "Example 4:\n",
      "\"\"\" Python Character Mapping Codec cp1250 generated from 'MAPPINGS/VENDORS/MICSFT/WINDOWS/CP1250.TXT' with gencodec.py.\n",
      "\n",
      "\"\"\"#\"\n",
      "\n",
      "import codecs\n",
      "\n",
      "### Codec APIs\n",
      "\n",
      "class Codec(codecs.Codec):\n",
      "\n",
      "    def encod...\n",
      "\n",
      "Example 5:\n",
      "#!/usr/bin/python\n",
      "# encoding: utf-8 -*-\n",
      "\n",
      "# Copyright: (c) 2013, Matthias Vogelgesang <matthias.vogelgesang@gmail.com>\n",
      "# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gp...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a few examples from the dataset\n",
    "examples = dataset.select(range(5))\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(example['content'][:200] + '...\\n')  # Print first 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "###############################################################################\n",
      "##\n",
      "##  Copyright (C) 2013-2014 Tavendo GmbH\n",
      "##\n",
      "##  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "##  y\n",
      "\n",
      "Generated Text:\n",
      "###############################################################################\n",
      "##\n",
      "##  Copyright (C) 2013-2014 Tavendo GmbH\n",
      "##\n",
      "##  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "##  y you may not use this file except in compliance with the License.\n",
      "##  You may obtain a copy of the License at\n",
      "##\n",
      "##  http://www.apache.org/licenses/LICENSE-2.0\n",
      "##\n",
      "##  Unless required by applicable law or agreed to in writing, software\n",
      "## \n"
     ]
    }
   ],
   "source": [
    "# Select an example to use as a prompt\n",
    "example_index = 0  # Change this to use a different example\n",
    "prompt = examples[example_index]['content'][:200]  # Use first 200 characters as prompt\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "# Generate text based on the prompt\n",
    "generated_text = generate_text(prompt)\n",
    "print(\"\\nGenerated Text:\")\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the `example_index` in the cell above to try different prompts from the dataset. You can also modify the `max_length` parameter in the `generate_text` function to control the length of the generated text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
